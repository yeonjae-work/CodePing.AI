# 상세설계서 (개발전)

---
**Database ID**: 21718a4c-52a1-80d3-b24f-e12f28e3d1cf
**Created**: 2025-06-19T12:46:00+00:00
**Last Edited**: 2025-06-26T13:26:00+00:00
**URL**: https://www.notion.so/21718a4c52a180d3b24fe12f28e3d1cf
---

## 데이터베이스 스키마

- **모듈 마스터**: relation
  - 연결 대상: `21b18a4c-52a1-802c-b71b-fe1b96bc35b0`
- **진행률**: number
- **상태**: select
  - 옵션: 계획, 설계중, 검토중, 승인완료, 수정필요, 미반영, MVP 완료, 설계완료, 개발중, 완료, 개발완료
- **예상작업시간**: number
- **개발LLM**: select
  - 옵션: o3, claude-4-sonnet
- **생성일**: date
- **개발자**: people
- **전체 프로젝트 마스터 DB**: relation
  - 연결 대상: `20018a4c-52a1-80ef-ba02-d83fd578ccdf`
- **프롬프트**: relation
  - 연결 대상: `21718a4c-52a1-80f6-9108-f924a9048aa3`
- **완료예정일**: date
- **우선순위**: select
  - 옵션: 높음, 보통, 낮음
- **이름**: title

## 데이터베이스 내용 (5개 항목)

### 1. DiffAnalyzer 모듈 상세설계서
- **모듈 마스터**: 1개 연결
- **진행률**: None
- **상태**: 설계완료
- **예상작업시간**: None
- **개발LLM**: claude-4-sonnet
- **생성일**: 2025-06-26
- **개발자**: YunJeong Kim(관리자)
- **전체 프로젝트 마스터 DB**: 1개 연결
- **프롬프트**: 1개 연결
- **우선순위**: 높음
- **이름**: DiffAnalyzer 모듈 상세설계서
- **링크**: [DiffAnalyzer 모듈 상세설계서](https://www.notion.so/DiffAnalyzer-21e18a4c52a1817cbb98cb54411f800e)

**📄 페이지 내용:**
  # 🏗️ 시스템 아키텍처
  ---
  ### 전체 구조도
  ```plain text
  [GitDataParser] → [DiffAnalyzer 모듈] → [DataStorage]
                         ↓
                 [코드 분석 및 의미 추출]
                         ↓
                [LLMService에서 활용]

  ```
  ### 기술 스택
  - **백엔드**: Python 3.11+, FastAPI, GitPython, python-diff-parser
  - **데이터베이스**: PostgreSQL, SQLAlchemy ORM
  - **AI/ML**: tree-sitter (코드 파싱), pygments (언어 감지), radon (복잡도 계산)
  - **기타**: Docker, Pydantic, pytest
  ---
  ## 📦 클래스 설계
  ### 클래스 1: DiffAnalyzer (메인 클래스)
  ### 역할과 책임
  - 모듈의 메인 진입점 및 전체 분석 프로세스 조율
  - GitDataParser에서 파싱된 diff 데이터를 받아 의미적 분석 수행
  - 코드 복잡도, 품질 메트릭, 구조적 변경사항 분석
  - 내부 헬퍼 클래스들을 조율하여 종합적인 코드 분석 결과 생성
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 필수여부 | 예시 |
  | --- | --- | --- | --- | --- |
  | parsed_diff | ParsedDiff | GitDataParser에서 파싱된 diff 데이터 | 필수 | ParsedDiff(...) |
  | commit_metadata | CommitMetadata | 커밋 메타데이터 | 필수 | CommitMetadata(...) |
  | repository_context | RepositoryContext | 저장소 컨텍스트 정보 | 선택 | RepositoryContext(...) |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | analyzed_files | List[AnalyzedFile] | 파일별 분석 결과 | [...] |
  | total_additions | int | 총 추가된 라인 수 | 127 |
  | total_deletions | int | 총 삭제된 라인 수 | 43 |
  | complexity_delta | float | 복잡도 변화량 | +1.2 |
  | language_breakdown | Dict[str, int] | 언어별 변경 라인 수 | {"python": 89, "js": 38} |
  | binary_files_changed | List[str] | 변경된 바이너리 파일 | ["image.png"] |
  | functions_added | List[str] | 추가된 함수명 | ["authenticate"] |
  | functions_modified | List[str] | 수정된 함수명 | ["login"] |
  ### 처리 플로우
  **초기화**
  1. 내부 헬퍼 클래스들 초기화
  1. 언어별 분석기 설정
  1. 복잡도 계산기 준비
  **데이터 처리**
  1. ParsedDiff 데이터 검증
  1. LanguageDetector로 언어별 분류
  1. CodeComplexityAnalyzer로 복잡도 분석
  1. StructuralChangeAnalyzer로 구조적 변경 분석
  1. 결과 통합 및 집계
  **결과 반환**
  1. 분석 결과 구조화
  1. 메트릭 집계
  1. DiffAnalysisResult 객체 반환
  ### 핵심 메서드
  - `analyze(parsed_diff: ParsedDiff, commit_metadata: CommitMetadata) -> DiffAnalysisResult`: 메인 분석 로직
  - `_orchestrate_analysis(parsed_diff: ParsedDiff) -> AnalysisResult`: 내부 분석 프로세스 조율
  - `_aggregate_results(file_analyses: List[FileAnalysis]) -> Summary`: 결과 집계
  ---
  ### 클래스 2: LanguageAnalyzer (헬퍼 클래스)
  ### 역할과 책임
  - 파싱된 파일 변경사항을 언어별로 분류
  - 언어별 특화된 분석 규칙 적용
  - 지원되지 않는 언어 필터링
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 필수여부 | 예시 |
  | --- | --- | --- | --- | --- |
  | file_changes | List[FileChange] | 파싱된 파일 변경사항 | 필수 | [...] |
  | repository_info | RepositoryInfo | 저장소 정보 | 필수 | RepositoryInfo(...) |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | language_groups | Dict[str, List[FileChange]] | 언어별 파일 그룹 | {"python": [...], "js": [...]} |
  | supported_files | List[FileChange] | 분석 지원 파일 목록 | [...] |
  | unsupported_files | List[FileChange] | 지원하지 않는 파일 목록 | [...] |
  ### 처리 플로우
  **초기화**
  1. 언어 매핑 테이블 로드
  1. 지원 언어 목록 설정
  **데이터 처리**
  1. 파일 확장자 기반 언어 분류
  1. 지원 여부 검증
  1. 언어별 그룹화
  **결과 반환**
  1. 언어별 파일 그룹 생성
  1. LanguageClassificationResult 반환
  ### 핵심 메서드
  - `classify_by_language(file_changes: List[FileChange]) -> LanguageClassificationResult`: 언어별 분류
  - `_detect_language(file_path: str) -> str`: 파일 언어 감지
  - `_is_supported_language(language: str) -> bool`: 지원 언어 여부 확인
  ---
  ### 클래스 3: CodeComplexityAnalyzer (헬퍼 클래스)
  ### 역할과 책임
  - 언어별 코드 복잡도 분석
  - 변경 전후 복잡도 비교
  - 복잡도 변화량 및 영향도 계산
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 필수여부 | 예시 |
  | --- | --- | --- | --- | --- |
  | file_change | FileChange | 파일 변경사항 (변경 전후 코드 포함) | 필수 | FileChange(...) |
  | language | str | 프로그래밍 언어 | 필수 | "python" |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | complexity_before | float | 변경 전 복잡도 | 2.1 |
  | complexity_after | float | 변경 후 복잡도 | 3.3 |
  | complexity_delta | float | 복잡도 변화량 | +1.2 |
  | impact_level | str | 영향도 수준 | "medium" |
  ### 처리 플로우
  **초기화**
  1. radon 복잡도 계산기 설정
  1. 언어별 가중치 로드
  **데이터 처리**
  1. 변경 전후 코드 복잡도 계산
  1. 델타 계산
  1. 언어별 가중치 적용
  **결과 반환**
  1. ComplexityMetrics 객체 반환
  ### 핵심 메서드
  - `analyze_complexity(file_change: FileChange, language: str) -> ComplexityAnalysisResult`: 복잡도 분석
  - `_calculate_single_complexity(code: str, language: str) -> float`: 단일 코드 복잡도 계산
  - `_determine_impact_level(delta: float) -> str`: 복잡도 변화 영향도 판정
  ---
  ### 클래스 4: StructuralChangeAnalyzer (헬퍼 클래스)
  ### 역할과 책임
  - AST 기반 구조적 변경사항 분석
  - 함수/클래스 레벨 변경 추적
  - 테스트 파일 변경 감지
  - Import/Dependency 변경 분석
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 필수여부 | 예시 |
  | --- | --- | --- | --- | --- |
  | file_change | FileChange | 파일 변경사항 | 필수 | FileChange(...) |
  | language | str | 프로그래밍 언어 | 필수 | "python" |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | functions_added | List[str] | 추가된 함수 | ["auth_user"] |
  | functions_modified | List[str] | 수정된 함수 | ["login"] |
  | functions_deleted | List[str] | 삭제된 함수 | ["old_login"] |
  | classes_added | List[str] | 추가된 클래스 | ["UserAuth"] |
  | classes_modified | List[str] | 수정된 클래스 | ["User"] |
  | imports_changed | List[str] | 변경된 import | ["requests"] |
  | is_test_file | bool | 테스트 파일 여부 | false |
  ### 처리 플로우
  **초기화**
  1. tree-sitter 파서 설정
  1. 함수/클래스 추출기 준비
  **데이터 처리**
  1. AST 파싱
  1. 함수/클래스 추출
  1. 변경사항 분류
  **결과 반환**
  1. FileAnalysis 객체 반환
  ### 핵심 메서드
  - `analyze_structural_changes(file_change: FileChange, language: str) -> StructuralAnalysisResult`: 구조적 변경 분석
  - `_extract_functions_from_ast(ast: ASTNode) -> List[Function]`: AST에서 함수 추출
  - `_extract_classes_from_ast(ast: ASTNode) -> List[Class]`: AST에서 클래스 추출
  - `_detect_test_file(file_path: str) -> bool`: 테스트 파일 감지
  - `_analyze_import_changes(before_ast: ASTNode, after_ast: ASTNode) -> List[str]`: Import 변경 분석
  ---
  ## 🔄 데이터 흐름도
  ### 전체 데이터 플로우
  ```plain text
  [GitDataParser] → [구조화된 diff 데이터] → [DiffAnalyzer 모듈] → [코드 분석 결과] → [DataStorage]

  ```
  ### 상세 데이터 흐름
  ### Step 1: 데이터 입력
  - **Source**: GitDataParser 모듈
  - **Format**: 구조화된 ParsedDiff 객체 + CommitMetadata
  - **Validation**: 파싱된 데이터 구조 검증
  ### Step 2: DiffAnalyzer 모듈 내부 처리
  - **LanguageAnalyzer**: 파일별 언어 분류 및 지원 여부 확인
  - **CodeComplexityAnalyzer**: 언어별 복잡도 분석
  - **StructuralChangeAnalyzer**: AST 기반 구조적 변경 분석
  - **DiffAnalyzer**: 분석 결과 통합 및 집계
  ### Step 3: 최종 출력
  - **Target**: DataStorage 모듈
  - **Format**: DiffAnalysisResult 객체 (코드 분석 결과)
  - **다음 단계**: LLMService에서 자연어 요약 생성에 활용
  ---
  ## 🔗 다른 모듈과의 연계
  ### Input 인터페이스
  - **GitDataParser**: 구조화된 diff 데이터 및 커밋 메타데이터 제공
  ### Output 인터페이스
  - **DataStorage**: 코드 분석 결과 저장을 위한 데이터 제공
  - **LLMService**: 자연어 요약 생성을 위한 분석된 코드 데이터 제공
  - **DataAggregator**: 개발자별/프로젝트별 집계를 위한 코드 분석 데이터 제공
  ### 데이터 모델 정의
  ```python
  @dataclass
  class DiffAnalysisResult:
      """DiffAnalyzer 모듈의 최종 출력 데이터 모델"""
      commit_sha: str
      repository_name: str
      author_email: str
      timestamp: datetime

      # 파일 변경 통계
      total_files_changed: int
      total_additions: int
      total_deletions: int

      # 언어별 분석
      language_breakdown: Dict[str, LanguageStats]

      # 구조적 변경사항
      functions_added: List[str]
      functions_modified: List[str]
      functions_deleted: List[str]
      classes_added: List[str]
      classes_modified: List[str]

      # 품질 메트릭
      complexity_delta: float
      test_coverage_delta: float

      # 파일별 상세 분석
      file_analyses: List[FileAnalysis]

      # 바이너리 파일 변경
      binary_files_changed: List[str]

  ```
  ---
  ## ⚙️ 구현 세부사항
  ### 환경 설정
  - **개발환경**: Python 3.11+, Poetry 패키지 관리
  - **의존성**:
  - **환경변수**:
  ### 핵심 로직
  ### 알고리즘 1: 언어별 파일 분류 알고리즘
  ```python
  def classify_files_by_language(file_changes: List[FileChange]) -> Dict[str, List[FileChange]]:
      """
      파싱된 파일 변경사항을 언어별로 분류
      """
      language_groups = {}

      for file_change in file_changes:
          language = detect_language_from_path(file_change.file_path)

          if is_supported_language(language):
              if language not in language_groups:
                  language_groups[language] = []
              language_groups[language].append(file_change)

      return language_groups

  ```
  ### 알고리즘 2: AST 기반 구조적 변경 분석
  ```python
  def analyze_structural_changes(file_change: FileChange, language: str) -> StructuralChanges:
      """
      AST를 이용한 함수/클래스 레벨 변경사항 분석
      """
      before_ast = parse_ast(file_change.content_before, language) if file_change.content_before else None
      after_ast = parse_ast(file_change.content_after, language) if file_change.content_after else None

      if not after_ast:  # 파일 삭제
          return StructuralChanges(deleted_functions=extract_functions(before_ast))

      if not before_ast:  # 새 파일
          return StructuralChanges(added_functions=extract_functions(after_ast))

      # 변경 분석
      before_functions = extract_functions(before_ast)
      after_functions = extract_functions(after_ast)

      added = [f for f in after_functions if f not in before_functions]
      deleted = [f for f in before_functions if f not in after_functions]
      modified = analyze_function_modifications(before_functions, after_functions)

      return StructuralChanges(
          added_functions=added,
          deleted_functions=deleted,
          modified_functions=modified
      )
  ```(before_code: str, after_code: str, language: str) -> float:
      """
      코드 변경 전후의 복잡도 변화량 계산
      """
      before_complexity = calculate_cyclomatic_complexity(before_code, language)
      after_complexity = calculate_cyclomatic_complexity(after_code, language)

      # 가중치 적용: 새로 추가된 코드에 더 높은 가중치
      if not before_code:  # 새 파일
          return after_complexity * 1.2
      elif not after_code:  # 삭제된 파일
          return -before_complexity
      else:  # 수정된 파일
          return after_complexity - before_complexity

  ```
  ### 성능 고려사항
  - **병목 지점**: 대용량 diff 파싱, tree-sitter AST 생성, 복잡도 계산
  - **최적화 방안**:
  - **확장성**: Redis 캐싱으로 분석 결과 저장, 워커 큐를 통한 분산 처리
  ---
  ## 🧪 테스트 계획
  ### 단위 테스트
  - ☐ **DiffAnalyzer** 메인 클래스 테스트
  - ☐ **DiffParser** 클래스 테스트
  - ☐ **LanguageDetector** 클래스 테스트
  - ☐ **ComplexityCalculator** 클래스 테스트
  - ☐ **FileChangeAnalyzer** 클래스 테스트
  ### 통합 테스트
  - ☐ GitDataParser → DiffAnalyzer → DataStorage 전체 플로우
  - ☐ 실제 프로젝트 diff 데이터를 이용한 종단간 테스트
  - ☐ 다른 모듈과의 인터페이스 호환성 테스트
  ### 성능 테스트
  - ☐ 대용량 diff (1000+ 파일) 처리 성능
  - ☐ 동시 요청 처리 능력 (100 req/sec)
  - ☐ 메모리 사용량 최적화 검증
  - ☐ AST 파싱 성능 벤치마크
  ### 테스트 데이터
  - **실제 프로젝트**: 다양한 크기의 오픈소스 프로젝트 diff
  - **언어 다양성**: Python, JavaScript, Java, Go, Rust 등
  - **특수 케이스**: 바이너리 파일, 대용량 파일, 특수 문자 포함 파일
  ---
  ## 📈 모니터링 및 로깅
  ### 핵심 메트릭
  - **처리 성능**: diff 파싱 시간, 분석 완료 시간, AST 생성 시간
  - **정확성**: 파싱 성공률, 언어 감지 정확도, 복잡도 계산 정확성
  - **시스템 리소스**: CPU 사용률, 메모리 사용량, 파일 I/O 성능
  ### 로그 레벨별 정보
  - **DEBUG**: 상세한 diff 파싱 과정, AST 구조, 복잡도 계산 세부사항
  - **INFO**: 분석 시작/완료, 주요 메트릭, 파일 처리 통계
  - **WARNING**: 파싱 실패, 지원하지 않는 언어, 대용량 파일 건너뛰기
  - **ERROR**: 시스템 오류, AST 파싱 실패, 메모리 부족
  ### 알림 및 경고 시스템
  - **성능 경고**: 처리 시간 임계값 초과 (30초 이상)
  - **품질 경고**: 복잡도 급증 감지 (이전 대비 50% 이상 증가)
  - **용량 경고**: 메모리 사용량 임계값 초과 (1GB 이상)
  ---
  ## 📚 참고 문서 및 의존성
  ### 외부 라이브러리 문서
  - **tree-sitter**: https://tree-sitter.github.io/tree-sitter/
  - **GitPython**: https://gitpython.readthedocs.io/
  - **radon**: https://radon.readthedocs.io/
  - **pygments**: https://pygments.org/docs/
  ### 코드 품질 기준
  - **PEP 8**: Python 코딩 스타일 가이드
  - **Cyclomatic Complexity**: 10 이하 권장, 15 이상 경고
  - **Test Coverage**: 신규 코드 80% 이상 커버리지 목표
  ### 보안 고려사항
  - **입력 검증**: diff 텍스트 크기 제한, 악성 코드 패턴 검사
  - **메모리 보호**: 메모리 사용량 모니터링, OOM 방지
  - **파일 접근**: 안전한 파일 경로 처리, 디렉토리 순회 공격 방지
  ---
  이 상세설계서는 DiffAnalyzer 모듈이 **순수한 분석 엔진**으로서 재사용 가능하고 확장 가능한 아키텍처를 제공하도록 설계되었습니다. 모듈 내부의 클래스들은 명확한 역할 분담을 통해 단일 책임 원칙을 준수하며, 다른 모듈과의 인터페이스를 통해 느슨한 결합을 유지합니다.

### 2. HTTPAPIClient 모듈 상세설계서
- **모듈 마스터**: 1개 연결
- **진행률**: None
- **상태**: 개발완료
- **예상작업시간**: None
- **개발LLM**: claude-4-sonnet
- **생성일**: 2025-06-26
- **개발자**: YunJeong Kim(관리자)
- **전체 프로젝트 마스터 DB**: 1개 연결
- **프롬프트**: 1개 연결
- **우선순위**: 높음
- **이름**: HTTPAPIClient 모듈 상세설계서
- **링크**: [HTTPAPIClient 모듈 상세설계서](https://www.notion.so/HTTPAPIClient-21e18a4c52a1814fb262e696d641ef59)

**📄 페이지 내용:**
  # HTTPAPIClient 모듈 상세설계서
  ## 🏗️ 시스템 아키텍처
  ---
  ### 전체 구조도
  ```plain text
  HTTPAPIClient (범용 HTTP 클라이언트)
      ↓
  [GitHub API Client] → [GitLab API Client] → [Generic API Client]
      ↓                      ↓                      ↓
  [API 응답 처리]     [API 응답 처리]     [API 응답 처리]

  ```
  ### 기술 스택
  - **백엔드**: Python 3.11+, aiohttp, httpx
  - **데이터베이스**: PostgreSQL (연결 정보 저장)
  - **기타**: Pydantic (데이터 검증), tenacity (재시도 로직), Redis (캐싱)
  ---
  ## 📦 클래스 설계
  ### 클래스 1: HTTPAPIClient
  ### 역할과 책임
  - GitHub API, GitLab API 등 외부 API 호출을 담당하는 범용 HTTP 클라이언트
  - webhook에서 추가 상세 정보가 필요한 경우 해당 플랫폼의 API를 호출하여 diff 상세정보 등을 가져옴
  - 인증, 재시도, 응답 처리, 에러 핸들링 등 HTTP 통신의 모든 측면을 관리
  - 다양한 Git 플랫폼 확장을 위한 범용 인터페이스 제공
  ### Input 데이터
  | 필드명 | 타입 | 필수 | 설명 | 예시 |
  | --- | --- | --- | --- | --- |
  | platform | str | Y | API 플랫폼 유형 | "github", "gitlab" |
  | endpoint | str | Y | API 엔드포인트 | "/repos/{owner}/{repo}/commits/{sha}" |
  | method | str | Y | HTTP 메서드 | "GET", "POST" |
  | headers | dict | N | 요청 헤더 | {"Authorization": "token xxx"} |
  | params | dict | N | 쿼리 매개변수 | {"per_page": 100} |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | status_code | int | HTTP 상태 코드 | 200 |
  | data | dict | 응답 데이터 | API 응답 JSON |
  | success | bool | 성공 여부 | True |
  | error_message | str | 에러 메시지 | "Rate limit exceeded" |
  ### 처리 플로우
  ## 초기화
  1. **환경 설정 로드**: API 키, 베이스 URL 등 플랫폼별 설정 로드
  1. **HTTP 클라이언트 초기화**: aiohttp 또는 httpx 클라이언트 세션 생성
  1. **재시도 정책 설정**: 네트워크 오류, Rate Limit 등에 대한 재시도 로직 설정
  ## 데이터 처리
  1. **요청 검증**: 입력 파라미터 유효성 검사 (Pydantic 사용)
  1. **플랫폼별 URL 구성**: 각 플랫폼의 API 베이스 URL과 엔드포인트 조합
  1. **인증 헤더 구성**: 플랫폼별 인증 방식에 따른 헤더 설정
  1. **HTTP 요청 실행**: 비동기 HTTP 요청 전송
  1. **응답 처리**: 상태 코드 확인, JSON 파싱, 에러 처리
  ## 결과 반환
  1. **응답 정규화**: 플랫폼별 응답을 통일된 형태로 변환
  1. **캐싱 처리**: 동일한 요청에 대한 결과 캐싱 (선택적)
  1. **로깅**: 요청/응답 정보 기록
  ### 핵심 메서드
  - `make_request()`: 범용 HTTP 요청 메서드
  - `get_github_diff()`: GitHub diff 정보 조회
  - `get_gitlab_diff()`: GitLab diff 정보 조회
  - `handle_rate_limit()`: Rate Limit 처리
  - `retry_request()`: 재시도 로직
  ---
  ### 클래스 2: PlatformAPIAdapter
  ### 역할과 책임
  - 각 플랫폼별 API 스펙의 차이를 추상화
  - 플랫폼별 인증 방식, URL 형식, 응답 형태를 통일된 인터페이스로 제공
  - 새로운 플랫폼 추가 시 확장 포인트 역할
  ### Input 데이터
  | 필드명 | 타입 | 필수 | 설명 | 예시 |
  | --- | --- | --- | --- | --- |
  | platform | str | Y | 플랫폼 유형 | "github" |
  | operation | str | Y | 수행할 작업 | "get_diff", "get_commit" |
  | repository | str | Y | 저장소 정보 | "owner/repo" |
  | commit_sha | str | Y | 커밋 해시 | "abc123def456" |
  | auth_token | str | Y | 인증 토큰 | "ghp_xxxxxxxxxxxx" |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | url | str | 구성된 API URL | "https://api.github.com/repos/..." |
  | headers | dict | 인증 헤더 | {"Authorization": "token xxx"} |
  | params | dict | 쿼리 매개변수 | {"per_page": 100} |
  | parser | callable | 응답 파서 함수 | github_diff_parser |
  ### 처리 플로우
  ## 초기화
  1. **플랫폼별 설정 로드**: 각 플랫폼의 API 스펙 정보 로드
  1. **어댑터 등록**: 지원하는 플랫폼별 어댑터 매핑 등록
  ## 데이터 처리
  1. **플랫폼 식별**: 입력된 플랫폼 유형에 따른 어댑터 선택
  1. **URL 구성**: 플랫폼별 API URL 패턴에 따른 URL 생성
  1. **인증 처리**: 플랫폼별 인증 방식 적용
  1. **파라미터 매핑**: 통일된 요청 파라미터를 플랫폼별 형식으로 변환
  ## 결과 반환
  1. **요청 정보 패키지**: URL, 헤더, 파라미터 등을 하나의 객체로 구성
  1. **파서 함수 제공**: 응답 데이터 파싱을 위한 함수 제공
  ### 핵심 메서드
  - `get_adapter()`: 플랫폼별 어댑터 반환
  - `build_url()`: API URL 구성
  - `get_auth_headers()`: 인증 헤더 생성
  - `parse_response()`: 응답 데이터 파싱
  ---
  ## 🔄 데이터 흐름도
  ### 전체 데이터 플로우
  ```plain text
  [Webhook Event] → [HTTPAPIClient] → [PlatformAPIAdapter] → [External API] → [Response Parser] → [Normalized Data]

  ```
  ### 상세 데이터 흐름
  ### Step 1: 데이터 입력
  - **Source**: Webhook 이벤트 또는 다른 모듈
  - **Format**: JSON 형태의 API 요청 정보
  - **Validation**: Pydantic 모델을 통한 입력 검증
  ### Step 2: 1차 처리
  - **Processor**: PlatformAPIAdapter
  - **Input**: 통일된 요청 형식
  - **Output**: 플랫폼별 HTTP 요청 정보
  - **Transform**: 플랫폼별 API 스펙에 맞는 변환
  ### Step 3: 2차 처리
  - **Processor**: HTTPAPIClient
  - **Input**: 플랫폼별 HTTP 요청 정보
  - **Output**: API 응답 데이터
  - **Transform**: HTTP 요청 실행 및 응답 처리
  ### Step 4: 최종 출력
  - **Target**: 다른 모듈 (DiffAnalyzer 등)
  - **Format**: 정규화된 JSON 응답
  - **Post-processing**: 캐싱, 로깅, 에러 처리
  ---
  ## 🔐 API 인증 관리
  ### 플랫폼별 인증 방식
  ### GitHub
  - **방식**: Personal Access Token (PAT)
  - **헤더**: `Authorization: token {token}`
  - **스코프**: `repo`, `read:user`
  ### GitLab
  - **방식**: Private Token 또는 OAuth
  - **헤더**: `PRIVATE-TOKEN: {token}`
  - **권한**: `read_api`, `read_repository`
  ### 보안 고려사항
  - **토큰 저장**: 환경변수 또는 보안 vault 사용
  - **토큰 로테이션**: 주기적 토큰 갱신
  - **권한 최소화**: 필요한 최소 권한만 부여
  - **감사 로그**: 모든 API 호출 기록
  ---
  ## 📊 Rate Limit 처리 전략
  ### 플랫폼별 제한사항
  ### GitHub
  - **인증된 요청**: 5,000 requests/hour
  - **검색 API**: 30 requests/minute
  - **헤더 정보**: `X-RateLimit-Remaining`, `X-RateLimit-Reset`
  ### GitLab
  - **일반 API**: 300 requests/minute
  - **검색 API**: 120 requests/minute
  - **헤더 정보**: `RateLimit-Remaining`, `RateLimit-Reset`
  ### 처리 전략
  1. **사전 확인**: 요청 전 남은 한도 체크
  1. **지수 백오프**: Rate limit 도달 시 대기
  1. **우선순위 큐**: 중요한 요청 우선 처리
  1. **분산 처리**: 여러 토큰 활용한 부하 분산
  ```python
  async def handle_rate_limit(response_headers):
      remaining = int(response_headers.get('X-RateLimit-Remaining', 0))
      reset_time = int(response_headers.get('X-RateLimit-Reset', 0))

      if remaining < 10:  # 임계값 설정
          wait_time = reset_time - time.time()
          await asyncio.sleep(max(wait_time, 0))

  ```
  ---
  ## 🔄 응답 캐싱 전략
  ### 캐싱 대상
  - **정적 데이터**: 커밋 정보, diff 데이터
  - **메타데이터**: 저장소 정보, 사용자 정보
  - **검색 결과**: 파일 검색, 브랜치 목록
  ### 캐싱 정책
  | 데이터 유형 | TTL | 캐시 키 | 설명 |
  | --- | --- | --- | --- |
  | 커밋 정보 | 24시간 | `commit:{platform}:{repo}:{sha}` | 변경되지 않는 데이터 |
  | diff 데이터 | 12시간 | `diff:{platform}:{repo}:{sha}` | 코드 변경사항 |
  | 사용자 정보 | 1시간 | `user:{platform}:{username}` | 프로필 정보 |
  | 저장소 정보 | 30분 | `repo:{platform}:{owner}:{repo}` | 메타데이터 |
  ### 캐시 무효화
  - **시간 기반**: TTL 만료
  - **이벤트 기반**: 새로운 커밋 감지
  - **수동 무효화**: 관리자 명령
  ---
  ## ⚙️ 구현 세부사항
  ### 환경 설정
  - **개발환경**: Python 3.11+, asyncio 기반 비동기 처리
  - **의존성**: aiohttp, httpx, pydantic, tenacity, python-dotenv, redis
  - **환경변수**: GITHUB_TOKEN, GITLAB_TOKEN, REDIS_URL, API_BASE_URLS
  ### 핵심 로직
  ### 알고리즘 1: 지수 백오프 재시도
  ```python
  async def exponential_backoff_retry(func, max_retries=3, base_delay=1):
      for attempt in range(max_retries):
          try:
              return await func()
          except RateLimitError:
              delay = base_delay * (2 ** attempt)
              await asyncio.sleep(delay)
          except Exception as e:
              if attempt == max_retries - 1:
                  raise e

  ```
  ### 알고리즘 2: 응답 정규화
  ```python
  def normalize_api_response(platform, response_data):
      if platform == "github":
          return GitHubResponseNormalizer(response_data).normalize()
      elif platform == "gitlab":
          return GitLabResponseNormalizer(response_data).normalize()
      else:
          return GenericResponseNormalizer(response_data).normalize()

  ```
  ### 성능 고려사항
  - **병목 지점**: API Rate Limit, 네트워크 지연, 대용량 diff 데이터
  - **최적화 방안**: 연결 풀링, 응답 캐싱, 병렬 요청 처리, 압축 활용
  - **확장성**: 플러그인 기반 플랫폼 어댑터, 수평 확장 가능한 아키텍처
  ---
  ## 🧪 테스트 계획
  ### 단위 테스트
  HTTPAPIClient 클래스 테스트
  PlatformAPIAdapter 클래스 테스트
  인증 처리 테스트
  Rate Limit 처리 테스트
  ### 통합 테스트
  GitHub API 연동 테스트
  GitLab API 연동 테스트
  캐싱 시스템 테스트
  에러 시나리오 테스트
  ### 성능 테스트
  Rate Limit 처리 성능 테스트
  대용량 diff 데이터 처리 테스트
  동시 요청 처리 테스트
  캐시 효율성 테스트
  ---
  ## 📊 확장 가능성
  ### 지원 예정 플랫폼
  - **Bitbucket**: API 어댑터 추가
  - **Azure DevOps**: 기업용 Git 플랫폼 지원
  - **Self-hosted Git**: 커스텀 Git 서버 지원
  ### 향후 기능
  - **GraphQL 지원**: GitHub GraphQL API 활용
  - **Webhook 검증**: 보안 강화를 위한 서명 검증
  - **메트릭 수집**: API 사용량, 성능 지표 추적
  - **Circuit Breaker**: 장애 전파 방지 패턴 적용
  ---
  ## 🔍 모니터링 및 로깅
  ### 핵심 메트릭
  - API 응답 시간 (평균, P95, P99)
  - 성공/실패율
  - Rate Limit 사용량
  - 캐시 히트율
  - 에러 발생 빈도
  ### 로그 레벨
  - **INFO**: 정상적인 API 호출, 캐시 히트/미스
  - **WARNING**: Rate Limit 근접, 재시도 발생, 느린 응답
  - **ERROR**: API 호출 실패, 인증 오류, 네트워크 에러
  - **DEBUG**: 상세한 요청/응답 정보, 헤더 정보
  ---
  ## 📝 설정 파일 예시
  ```yaml
  # config/http_api_client.yaml
  platforms:
    github:
      base_url: "https://api.github.com"
      auth_header: "Authorization"
      auth_prefix: "token"
      rate_limit:
        requests_per_hour: 5000
        search_requests_per_minute: 30

    gitlab:
      base_url: "https://gitlab.com/api/v4"
      auth_header: "PRIVATE-TOKEN"
      rate_limit:
        requests_per_minute: 300

  retry_policy:
    max_retries: 3
    base_delay: 1
    max_delay: 60
    backoff_factor: 2

  cache:
    enabled: true
    backend: "redis"
    default_ttl: 300  # 5분
    max_size: 10000

  connection_pool:
    max_connections: 100
    max_keepalive_connections: 20
    keepalive_expiry: 5

  ```
  이 설계서는 HTTPAPIClient 모듈이 CodePing.AI 프로젝트에서 안정적이고 효율적인 외부 API 통신을 담당하도록 설계되었습니다.

### 3. DataStorage 모듈 상세설계서(설계서수정필요함)
- **모듈 마스터**: 1개 연결
- **진행률**: None
- **상태**: MVP 완료
- **예상작업시간**: None
- **개발LLM**: claude-4-sonnet
- **생성일**: 2025-06-26
- **개발자**: YunJeong Kim(관리자)
- **전체 프로젝트 마스터 DB**: 1개 연결
- **프롬프트**: 1개 연결
- **이름**: DataStorage 모듈 상세설계서(설계서수정필요함)
- **링크**: [DataStorage 모듈 상세설계서(설계서수정필요함)](https://www.notion.so/DataStorage-21e18a4c52a181f1b742f30c4b67dfe5)

**📄 페이지 내용:**
  # 🏗️ 시스템 아키텍처
  ---
  ### 전체 구조도
  ```plain text
  [WebhookReceiver] → [GitDataParser] → [DataStorage] → [PostgreSQL DB]
                                             ↓
                                     [Event Sourcing Store]
                                             ↓
                                     [ActivityAnalyzer]

  ```
  ### 기술 스택
  - **백엔드**: Python 3.11+, FastAPI, SQLAlchemy
  - **데이터베이스**: PostgreSQL 17 (벡터 검색 지원, 향상된 JSONB 인덱싱)
  - **캐싱**: Redis 7.4 (멀티스레드 처리, 향상된 메모리 관리)
  - **메시지 큐**: Apache Kafka (이벤트 스트리밍)
  - **컨테이너**: Docker, Docker Compose
  - **모니터링**: Prometheus, Grafana
  - **AI/ML**: OpenAI GPT-4 API
  ---
  ## 📦 클래스 설계
  ### 클래스 1: DataStorageManager
  ### 역할과 책임
  - 구조화된 커밋 데이터의 영구 저장 관리
  - 데이터베이스 트랜잭션 및 무결성 보장
  - 이벤트 소싱 패턴 구현을 통한 변경 이력 추적
  - 데이터 검증 및 변환 처리
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 제약조건 | 예시 |
  | --- | --- | --- | --- | --- |
  | commit_data | CommitData | 구조화된 커밋 정보 | 필수, 검증됨 | 커밋 해시, 메시지, 작성자 등 |
  | diff_data | DiffData | 코드 변경사항 정보 | 필수 | 파일별 추가/삭제/수정 라인 |
  | metadata | Dict | 추가 메타데이터 | 선택적 | 타임스탬프, 브랜치 정보 |
  | user_id | str | 개발자 식별자 | 필수 | GitHub username |
  | repository_id | str | 저장소 식별자 | 필수 | repo owner/name |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 반환조건 |
  | --- | --- | --- | --- |
  | storage_id | UUID | 저장된 데이터 고유 ID | 성공 시 |
  | status | StorageStatus | 저장 상태 | 항상 |
  | timestamp | datetime | 저장 완료 시간 | 성공 시 |
  | error_details | Optional[str] | 오류 상세 정보 | 실패 시 |
  ### 처리 플로우
  **초기화**
  - PostgreSQL 연결 풀 설정 및 검증
  - Redis 캐시 연결 확인
  - 테이블 스키마 검증
  - 이벤트 소싱 스토어 초기화
  **데이터 처리**
  - 입력 데이터 스키마 검증 (Pydantic)
  - 중복 커밋 데이터 확인 (해시 기반)
  - 트랜잭션 시작
  - 메인 커밋 데이터 저장
  - 관련 diff 데이터 저장
  - 이벤트 생성 및 발행
  - 트랜잭션 커밋
  **결과 반환**
  - 저장 성공/실패 상태 반환
  - 생성된 저장소 ID 제공
  - 오류 발생 시 롤백 및 상세 로깅
  ### 핵심 메서드
  - `store_commit_data(commit_data: CommitData)`: 커밋 데이터 저장
  - `store_diff_data(diff_data: DiffData, commit_id: UUID)`: diff 데이터 저장
  - `validate_data_integrity(data: Any)`: 데이터 무결성 검증
  - `create_storage_event(data: Dict)`: 저장 이벤트 생성
  - `check_duplicate_commit(commit_hash: str)`: 중복 커밋 확인
  ---
  ### 클래스 2: EventSourcingStore
  ### 역할과 책임
  - 데이터 변경 이벤트의 순차적 저장
  - 이벤트 스트림 관리 및 재생 기능
  - 데이터 상태 재구성 지원
  - 감사 로그 및 추적성 제공
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 제약조건 | 예시 |
  | --- | --- | --- | --- | --- |
  | event_type | EventType | 이벤트 유형 | 필수 | COMMIT_STORED, DIFF_ADDED |
  | aggregate_id | UUID | 집계 식별자 | 필수 | 커밋 ID |
  | event_data | Dict | 이벤트 데이터 | 필수 | 변경된 데이터 |
  | version | int | 이벤트 버전 | 자동 증가 | 1, 2, 3... |
  | correlation_id | UUID | 연관 이벤트 ID | 선택적 | 관련 이벤트 추적용 |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 반환조건 |
  | --- | --- | --- | --- |
  | event_id | UUID | 이벤트 고유 ID | 성공 시 |
  | sequence_number | int | 시퀀스 번호 | 성공 시 |
  | stored_at | datetime | 저장 시간 | 성공 시 |
  | stream_position | int | 스트림 내 위치 | 성공 시 |
  ### 처리 플로우
  **초기화**
  - 이벤트 스토어 테이블 초기화
  - 시퀀스 번호 관리 설정
  - 스트림 파티셔닝 구성
  **데이터 처리**
  - 이벤트 직렬화 (JSON)
  - 스키마 버전 관리
  - 이벤트 순서 보장
  - 원자적 저장 처리
  - 스트림 인덱싱
  **결과 반환**
  - 이벤트 저장 확인
  - 스트림 위치 정보 반환
  - 구독자에게 이벤트 발행
  ### 핵심 메서드
  - `append_event(event: Event)`: 이벤트 추가
  - `get_events_by_aggregate(aggregate_id: UUID)`: 집계별 이벤트 조회
  - `replay_events(from_sequence: int)`: 이벤트 재생
  - `subscribe_to_stream(handler: Callable)`: 스트림 구독
  - `get_current_version(aggregate_id: UUID)`: 현재 버전 조회
  ---
  ### 클래스 3: DatabaseRepository
  ### 역할과 책임
  - 데이터베이스 CRUD 연산 추상화
  - 쿼리 최적화 및 성능 관리
  - 연결 풀 관리
  - 데이터 접근 계층 통합
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 제약조건 | 예시 |
  | --- | --- | --- | --- | --- |
  | entity | BaseModel | 저장할 엔티티 | 필수 | Commit, DiffEntry |
  | query_params | QueryParams | 조회 매개변수 | 선택적 | 필터, 정렬, 페이징 |
  | transaction_ctx | TransactionContext | 트랜잭션 컨텍스트 | 선택적 | 활성 트랜잭션 |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 반환조건 |
  | --- | --- | --- | --- |
  | result | List[BaseModel] | 조회 결과 | 조회 시 |
  | affected_rows | int | 영향받은 행 수 | 수정/삭제 시 |
  | execution_time | float | 실행 시간 (ms) | 항상 |
  ### 처리 플로우
  **초기화**
  - SQLAlchemy 엔진 설정
  - 연결 풀 구성 (최소 5, 최대 20)
  - 세션 팩토리 생성
  **데이터 처리**
  - 세션 생성 및 관리
  - 쿼리 실행 및 최적화
  - 트랜잭션 관리
  - 예외 처리 및 롤백
  **결과 반환**
  - 결과 객체 매핑
  - 성능 메트릭 수집
  - 리소스 정리
  ### 핵심 메서드
  - `save(entity: BaseModel)`: 엔티티 저장
  - `find_by_id(entity_id: UUID)`: ID로 조회
  - `find_by_criteria(criteria: Dict)`: 조건부 조회
  - `update(entity: BaseModel)`: 엔티티 수정
  - `delete(entity_id: UUID)`: 엔티티 삭제
  ---
  ## 🔄 데이터 흐름도
  ### 전체 데이터 플로우
  ```plain text
  [GitDataParser] → [DataStorageManager] → [PostgreSQL] → [EventSourcingStore] → [Kafka] → [ActivityAnalyzer]

  ```
  ### 상세 데이터 흐름
  ### Step 1: 데이터 입력
  - **Source**: GitDataParser에서 구조화된 커밋 데이터
  - **Format**: CommitData + DiffData (Pydantic 모델)
  - **Validation**: 스키마 검증, 필수 필드 확인, 데이터 타입 검증
  ### Step 2: 1차 처리
  - **Processor**: DataStorageManager
  - **Input**: 검증된 커밋 및 diff 데이터
  - **Output**: 저장된 데이터 ID 및 상태
  - **Transform**:
  ### Step 3: 2차 처리
  - **Processor**: EventSourcingStore
  - **Input**: 데이터 변경 이벤트
  - **Output**: 이벤트 스트림 위치
  - **Transform**:
  ### Step 4: 최종 출력
  - **Target**:
  - **Format**: 정규화된 관계형 데이터 + 이벤트 스트림
  - **Post-processing**:
  ---
  ## ⚙️ 구현 세부사항
  ### 환경 설정
  - **개발환경**: Python 3.11+, Poetry 의존성 관리
  - **의존성**:
  - **환경변수**:
  ### 핵심 로직
  ### 알고리즘 1: 데이터 저장 트랜잭션
  ```python
  async def store_commit_with_transaction(self, commit_data: CommitData, diff_data: DiffData) -> StorageResult:
      async with self.db.transaction() as txn:
          try:
              # 1. 중복 확인
              if await self.check_duplicate_commit(commit_data.hash):
                  return StorageResult(status=DUPLICATE, message="Commit already exists")

              # 2. 커밋 데이터 저장
              commit_id = await self.save_commit(commit_data, txn)

              # 3. Diff 데이터 저장
              diff_entries = await self.save_diff_data(diff_data, commit_id, txn)

              # 4. 이벤트 생성
              event = self.create_storage_event(commit_id, commit_data, diff_entries)
              await self.event_store.append_event(event)

              # 5. 캐시 업데이트
              await self.update_cache(commit_id, commit_data)

              await txn.commit()
              return StorageResult(status=SUCCESS, storage_id=commit_id)

          except Exception as e:
              await txn.rollback()
              logger.error(f"Storage transaction failed: {e}")
              return StorageResult(status=ERROR, error=str(e))

  ```
  ### 알고리즘 2: 이벤트 소싱 스트림 관리
  ```python
  async def append_event_with_ordering(self, event: Event) -> EventResult:
      # 1. 이벤트 검증
      self.validate_event_schema(event)

      # 2. 시퀀스 번호 생성 (Redis 원자적 증가)
      sequence_number = await self.redis.incr(f"seq:{event.aggregate_id}")

      # 3. 이벤트 직렬화
      serialized_event = {
          "event_id": str(uuid4()),
          "aggregate_id": str(event.aggregate_id),
          "event_type": event.type.value,
          "data": event.data,
          "version": sequence_number,
          "timestamp": datetime.utcnow().isoformat(),
          "metadata": event.metadata
      }

      # 4. 데이터베이스 저장
      event_id = await self.db.save_event(serialized_event)

      # 5. Kafka 발행
      await self.kafka_producer.send(
          topic=f"events.{event.aggregate_type}",
          value=serialized_event,
          key=str(event.aggregate_id)
      )

      return EventResult(event_id=event_id, sequence_number=sequence_number)

  ```
  ### 성능 고려사항
  - **병목 지점**:
  - **최적화 방안**:
  - **확장성**:
  ---
  ## 🧪 테스트 계획
  ### 단위 테스트
  DataStorageManager 클래스 테스트
  EventSourcingStore 클래스 테스트
  DatabaseRepository 클래스 테스트
  ### 통합 테스트
  전체 데이터 플로우 테스트
  외부 시스템 연동 테스트 (PostgreSQL, Redis, Kafka)
  장애 복구 테스트
  대용량 데이터 처리 테스트
  ### 성능 테스트
  동시 사용자 1000명 기준 처리량 테스트
  메모리 사용량 모니터링
  응답 시간 SLA 검증 (< 100ms)
  ---
  ## 📊 모니터링 및 알림
  ### 주요 메트릭
  - **성능 메트릭**:
  - **비즈니스 메트릭**:
  - **시스템 메트릭**:
  ### 알림 설정
  - **Critical**: 데이터베이스 연결 실패, 트랜잭션 실패율 > 5%
  - **Warning**: 응답 시간 > 200ms, 메모리 사용률 > 80%
  - **Info**: 일일 처리량 리포트, 성능 개선 제안
  ---
  ## 🔐 보안 및 규정 준수
  ### 데이터 보안
  - **암호화**:
  - **접근 제어**:
  ### 감사 및 규정 준수
  - **감사 로그**: 모든 데이터 변경 이벤트 기록
  - **데이터 보존**: 7년간 이벤트 스트림 보관
  - **개인정보 보호**: 개발자 식별 정보 해싱 처리
  ### 재해 복구
  - **백업 전략**:
  - **복구 목표**:
  ---
  ## 🚀 배포 및 운영
  ### 배포 전략
  - **Blue-Green 배포**: 무중단 서비스 제공
  - **카나리 배포**: 점진적 트래픽 전환 (10% → 50% → 100%)
  - **롤백 전략**: 1분 내 이전 버전 복구
  ### 운영 자동화
  - **헬스 체크**: `/health` 엔드포인트 (응답 시간, 의존성 상태)
  - **Auto Scaling**: CPU 80% 초과 시 인스턴스 증가
  - **장애 격리**: Circuit Breaker 패턴 적용
  ### 유지보수
  - **정기 작업**:
  - **업데이트 절차**:
  ---
  ## 🚀 MVP (Minimum Viable Product) 버전 설계
  ### MVP 범위 정의
  **포함 기능**
  - ✅ 기본 커밋 데이터 저장 (PostgreSQL)
  - ✅ 간단한 diff 데이터 저장
  - ✅ 중복 커밋 확인
  - ✅ 기본 트랜잭션 처리
  - ✅ 저장 성공/실패 응답
  **제외 기능 (향후 단계)**
  - ❌ 이벤트 소싱 (단순 CRUD로 시작)
  - ❌ Redis 캐싱 (데이터베이스만 사용)
  - ❌ Kafka 메시징 (직접 호출)
  - ❌ 복잡한 모니터링 (기본 로깅만)
  - ❌ 샤딩/분산 처리
  ### MVP 아키텍처
  ```plain text
  [GitDataParser] → [DataStorageManager] → [PostgreSQL]
                         ↓
                   [Simple Response]

  ```
  ### MVP 클래스 설계
  ### SimplifiedDataStorage
  ```python
  from sqlalchemy.orm import Session
  from pydantic import BaseModel
  from typing import Optional
  import logging

  class CommitData(BaseModel):
      commit_hash: str
      message: str
      author: str
      timestamp: datetime
      repository: str
      branch: str

  class DiffData(BaseModel):
      file_path: str
      additions: int
      deletions: int
      changes: str

  class StorageResult(BaseModel):
      success: bool
      commit_id: Optional[int] = None
      message: str

  class SimplifiedDataStorage:
      def __init__(self, db_session: Session):
          self.db = db_session
          self.logger = logging.getLogger(__name__)

      def store_commit(self, commit_data: CommitData, diff_data: List[DiffData]) -> StorageResult:
          """MVP: 간단한 커밋 데이터 저장"""
          try:
              # 1. 중복 확인
              if self._is_duplicate_commit(commit_data.commit_hash):
                  return StorageResult(
                      success=False,
                      message="Commit already exists"
                  )

              # 2. 커밋 저장
              commit_record = CommitRecord(
                  hash=commit_data.commit_hash,
                  message=commit_data.message,
                  author=commit_data.author,
                  timestamp=commit_data.timestamp,
                  repository=commit_data.repository,
                  branch=commit_data.branch
              )
              self.db.add(commit_record)
              self.db.flush()  # ID 생성

              # 3. diff 데이터 저장
              for diff in diff_data:
                  diff_record = DiffRecord(
                      commit_id=commit_record.id,
                      file_path=diff.file_path,
                      additions=diff.additions,
                      deletions=diff.deletions,
                      changes=diff.changes
                  )
                  self.db.add(diff_record)

              # 4. 커밋
              self.db.commit()

              self.logger.info(f"Stored commit: {commit_data.commit_hash}")
              return StorageResult(
                  success=True,
                  commit_id=commit_record.id,
                  message="Commit stored successfully"
              )

          except Exception as e:
              self.db.rollback()
              self.logger.error(f"Failed to store commit: {e}")
              return StorageResult(
                  success=False,
                  message=f"Storage failed: {str(e)}"
              )

      def _is_duplicate_commit(self, commit_hash: str) -> bool:
          """중복 커밋 확인"""
          return self.db.query(CommitRecord).filter(
              CommitRecord.hash == commit_hash
          ).first() is not None

  ```
  ### MVP 데이터베이스 스키마
  ```sql
  -- 커밋 테이블
  CREATE TABLE commits (
      id SERIAL PRIMARY KEY,
      hash VARCHAR(40) UNIQUE NOT NULL,
      message TEXT NOT NULL,
      author VARCHAR(255) NOT NULL,
      timestamp TIMESTAMP NOT NULL,
      repository VARCHAR(255) NOT NULL,
      branch VARCHAR(255) NOT NULL,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  );

  -- diff 테이블
  CREATE TABLE commit_diffs (
      id SERIAL PRIMARY KEY,
      commit_id INTEGER REFERENCES commits(id) ON DELETE CASCADE,
      file_path TEXT NOT NULL,
      additions INTEGER DEFAULT 0,
      deletions INTEGER DEFAULT 0,
      changes TEXT,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  );

  -- 인덱스
  CREATE INDEX idx_commits_hash ON commits(hash);
  CREATE INDEX idx_commits_timestamp ON commits(timestamp);
  CREATE INDEX idx_commits_author ON commits(author);
  CREATE INDEX idx_diffs_commit_id ON commit_diffs(commit_id);

  ```
  ### MVP 의존성 (requirements.txt)
  ```plain text
  # 핵심 의존성만
  fastapi==0.104.1
  sqlalchemy==2.0.23
  psycopg2-binary==2.9.7
  pydantic==2.5.0
  python-dotenv==1.0.0
  uvicorn==0.24.0

  # 개발/테스트
  pytest==7.4.3
  pytest-asyncio==0.21.1

  ```
  ### MVP 환경 설정
  ```python
  # settings.py
  from pydantic_settings import BaseSettings

  class Settings(BaseSettings):
      database_url: str = "postgresql://user:pass@localhost:5432/codeping"
      log_level: str = "INFO"

      class Config:
          env_file = ".env"

  settings = Settings()

  ```
  ### MVP 테스트 전략
  ```python
  # test_mvp_storage.py
  import pytest
  from sqlalchemy import create_engine
  from sqlalchemy.orm import sessionmaker
  from datetime import datetime

  @pytest.fixture
  def db_session():
      engine = create_engine("sqlite:///:memory:")  # 테스트용 인메모리 DB
      Base.metadata.create_all(engine)
      Session = sessionmaker(bind=engine)
      session = Session()
      yield session
      session.close()

  def test_store_commit_success(db_session):
      storage = SimplifiedDataStorage(db_session)

      commit_data = CommitData(
          commit_hash="abc123",
          message="Add new feature",
          author="john@example.com",
          timestamp=datetime.now(),
          repository="owner/repo",
          branch="main"
      )

      diff_data = [
          DiffData(
              file_path="src/main.py",
              additions=10,
              deletions=2,
              changes="Added new function"
          )
      ]

      result = storage.store_commit(commit_data, diff_data)

      assert result.success is True
      assert result.commit_id is not None
      assert "successfully" in result.message

  def test_duplicate_commit_prevention(db_session):
      storage = SimplifiedDataStorage(db_session)

      # 첫 번째 저장
      commit_data = CommitData(
          commit_hash="duplicate123",
          message="Test commit",
          author="test@example.com",
          timestamp=datetime.now(),
          repository="test/repo",
          branch="main"
      )

      result1 = storage.store_commit(commit_data, [])
      assert result1.success is True

      # 중복 저장 시도
      result2 = storage.store_commit(commit_data, [])
      assert result2.success is False
      assert "already exists" in result2.message

  ```
  ### MVP 배포 설정
  ```plain text
  # Dockerfile (MVP)
  FROM python:3.11-slim

  WORKDIR /app

  # 최소 의존성만 설치
  COPY requirements.txt .
  RUN pip install --no-cache-dir -r requirements.txt

  COPY . .

  EXPOSE 8000

  CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

  ```
  ```yaml
  # docker-compose.yml (MVP)
  version: '3.8'

  services:
    app:
      build: .
      ports:
        - "8000:8000"
      environment:
        - DATABASE_URL=postgresql://postgres:password@db:5432/codeping
      depends_on:
        - db

    db:
      image: postgres:15
      environment:
        - POSTGRES_DB=codeping
        - POSTGRES_USER=postgres
        - POSTGRES_PASSWORD=password
      ports:
        - "5432:5432"
      volumes:
        - postgres_data:/var/lib/postgresql/data

  volumes:
    postgres_data:

  ```
  ### MVP 개발 일정
  | 단계 | 작업 내용 | 예상 시간 | 담당자 |
  | --- | --- | --- | --- |
  | 1단계 | 데이터베이스 스키마 설계 및 생성 | 2시간 | Backend Dev |
  | 2단계 | 기본 모델 및 저장 로직 구현 | 4시간 | Backend Dev |
  | 3단계 | API 엔드포인트 구현 | 2시간 | Backend Dev |
  | 4단계 | 단위 테스트 작성 | 3시간 | Backend Dev |
  | 5단계 | 통합 테스트 및 디버깅 | 3시간 | Backend Dev |
  | 6단계 | Docker 설정 및 배포 | 2시간 | DevOps |
  **총 예상 시간: 16시간 (2일)**
  ### MVP에서 전체 버전으로의 마이그레이션 계획
  ### Phase 1: MVP → 기본 확장 (1주)
  - Redis 캐싱 추가
  - 기본 모니터링 (Prometheus metrics)
  - 에러 핸들링 강화
  ### Phase 2: 중급 확장 (2주)
  - 이벤트 소싱 도입
  - Kafka 메시징 추가
  - 성능 최적화
  ### Phase 3: 고급 확장 (3주)
  - 분산 처리
  - 고급 모니터링
  - 보안 강화
  ### MVP 성공 지표
  **기술적 지표**
  - ✅ 커밋 저장 성공률 > 99%
  - ✅ 응답 시간 < 500ms
  - ✅ 중복 커밋 정확한 탐지
  **비즈니스 지표**
  - ✅ 일일 100+ 커밋 처리 가능
  - ✅ 시스템 다운타임 < 1%
  - ✅ 개발팀 만족도 확인
  ---
  *이 설계서는 2025년 최신 데이터 저장 기술 트렌드를 반영하여 작성되었으며, 이벤트 소싱, 마이크로서비스 아키텍처, PostgreSQL 17의 새로운 기능들을 활용합니다. MVP 버전을 통해 빠른 검증과 점진적 확장이 가능하도록 설계되었습니다.*

### 4. GitDataParser 모듈 상세설계서 - CodePing.A
- **모듈 마스터**: 1개 연결
- **진행률**: 1
- **상태**: MVP 완료
- **예상작업시간**: 48
- **개발LLM**: claude-4-sonnet
- **생성일**: 2025-06-25
- **개발자**: YunJeong Kim(관리자)
- **전체 프로젝트 마스터 DB**: 1개 연결
- **프롬프트**: 1개 연결
- **완료예정일**: 2025-06-27
- **우선순위**: 높음
- **이름**: GitDataParser 모듈 상세설계서 - CodePing.A
- **링크**: [GitDataParser 모듈 상세설계서 - CodePing.A](https://www.notion.so/GitDataParser-CodePing-A-21d18a4c52a181d88c85db15b9cc8089)

**📄 페이지 내용:**
  # 🏗️ 시스템 아키텍처
  ---
  ## 전체 구조도
  ```plain text
  [WebhookReceiver] → [GitDataParser] → [DiffAnalyzer] → [DataStorage]
           ↓               ↓               ↓              ↓
      Raw Webhook   Structured Git   Parsed Diff    Database
         Data          Metadata       Analysis        Storage

  ```
  ## 기술 스택
  - **프론트엔드**: N/A (백엔드 모듈)
  - **백엔드**: Python 3.11+, FastAPI
  - **데이터베이스**: PostgreSQL, SQLAlchemy ORM
  - **AI/ML**: OpenAI GPT-4 API (downstream 모듈에서 사용)
  - **기타**: GitPython, python-diff-parser, tree-sitter, pygments, Pydantic
  ---
  # 📦 클래스 설계
  ## 클래스 1: GitDataParser
  ### 역할과 책임
  - GitHub Webhook payload에서 Git 커밋 메타데이터 추출
  - 커밋 정보, 작성자, 변경 파일 목록 구조화
  - Raw webhook 데이터를 분석 가능한 형태로 변환
  - Diff 정보 추출 및 기본 파싱 수행
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 예시 | 필수여부 |
  | --- | --- | --- | --- | --- |
  | webhook_payload | Dict | GitHub webhook 원시 데이터 | {"repository": {...}, "commits": [...]} | Required |
  | event_type | str | Webhook 이벤트 타입 | "push", "pull_request" | Required |
  | timestamp | datetime | 이벤트 발생 시간 | 2025-06-25T08:00:00Z | Required |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | repository_info | RepositoryInfo | 저장소 정보 | {name, url, owner} |
  | commits_data | List[CommitData] | 커밋 정보 목록 | [{sha, message, author, ...}] |
  | diff_metadata | DiffMetadata | Diff 메타데이터 | {files_changed, lines_added, lines_deleted} |
  ### 처리 플로우
  ## 초기화
  1. GitPython 라이브러리 초기화
  1. Diff parser 설정
  1. 지원 파일 확장자 목록 로드
  ## 데이터 처리
  1. Webhook payload 검증 및 파싱
  1. Repository 정보 추출
  1. Commit 목록 반복 처리
  1. 각 커밋별 diff 정보 추출
  1. 파일별 변경사항 분석
  ## 결과 반환
  1. 구조화된 CommitData 객체 생성
  1. DiffMetadata 집계
  1. RepositoryInfo 정리
  1. 검증된 결과 반환
  ### 핵심 메서드
  - `parse_webhook_data()`: Webhook payload 파싱 및 기본 정보 추출
  - `extract_commit_info()`: 개별 커밋 정보 추출
  - `parse_diff_data()`: Diff 정보 파싱 및 구조화
  - `validate_data()`: 추출된 데이터 검증
  ---
  ## 클래스 2: DiffProcessor
  ### 역할과 책임
  - 상세 diff 정보 파싱 및 분석
  - 프로그래밍 언어별 코드 변경사항 해석
  - 바이너리 파일 변경 감지
  - 코드 복잡도 계산 및 메트릭 생성
  ### Input 데이터
  | 필드명 | 타입 | 설명 | 예시 | 필수여부 |
  | --- | --- | --- | --- | --- |
  | raw_diff | str | Git diff 원시 텍스트 | "@@ -1,4 +1,6 @@\n..." | Required |
  | file_path | str | 변경된 파일 경로 | "src/auth/models.py" | Required |
  | language | str | 프로그래밍 언어 | "python", "javascript" | Optional |
  ### Output 데이터
  | 필드명 | 타입 | 설명 | 예시 |
  | --- | --- | --- | --- |
  | file_diff | FileDiff | 파일별 변경사항 | {additions, deletions, chunks} |
  | code_analysis | CodeAnalysis | 코드 분석 결과 | {complexity, patterns, quality} |
  | language_metrics | LanguageMetrics | 언어별 메트릭 | {functions_added, imports_changed} |
  ### 처리 플로우
  ## 초기화
  1. Tree-sitter 파서 초기화
  1. 언어별 분석 규칙 로드
  1. 코드 복잡도 분석기 설정
  ## 데이터 처리
  1. Diff 텍스트 파싱
  1. 변경 블록(hunk) 분리
  1. 언어 감지 및 구문 분석
  1. 추가/삭제/수정 라인 분류
  1. 코드 패턴 및 복잡도 분석
  ## 결과 반환
  1. FileDiff 객체 생성
  1. 언어별 메트릭 계산
  1. 코드 품질 지표 생성
  1. 분석 결과 검증 및 반환
  ### 핵심 메서드
  - `parse_diff_hunks()`: Diff hunk 파싱 및 라인별 분석
  - `analyze_code_changes()`: 코드 변경사항 의미론적 분석
  - `calculate_complexity()`: 순환 복잡도 및 품질 메트릭 계산
  - `detect_patterns()`: 코드 패턴 및 아키텍처 변경 감지
  ---
  # 🔄 데이터 흐름도
  ## 전체 데이터 플로우
  ```plain text
  [Webhook Input] → [GitDataParser] → [DiffProcessor] → [Structured Output]

  ```
  ## 상세 데이터 흐름
  ### Step 1: 데이터 입력
  - **Source**: GitHub Webhook API
  - **Format**: JSON payload with repository, commits, and diff data
  - **Validation**: Schema validation, required fields check
  ### Step 2: 1차 처리
  - **Processor**: GitDataParser
  - **Input**: Raw webhook payload
  - **Output**: Structured commit metadata + basic diff info
  - **Transform**: JSON → Pydantic models
  ### Step 3: 2차 처리
  - **Processor**: DiffProcessor
  - **Input**: Raw diff strings from commits
  - **Output**: Analyzed diff data with code metrics
  - **Transform**: Raw diff → FileDiff objects with analysis
  ### Step 4: 최종 출력
  - **Target**: DataStorage module (downstream)
  - **Format**: Structured CommitData + DiffAnalysis objects
  - **Post-processing**: Data validation, relationship mapping
  ---
  # ⚙️ 구현 세부사항
  ## 환경 설정
  - **개발환경**: Python 3.11+, FastAPI development server
  - **의존성**:
  - **환경변수**:
  ## 핵심 로직
  ### 알고리즘 1: Webhook Data Parsing
  ```python
  def parse_webhook_data(webhook_payload: Dict) -> ParsedWebhookData:
      """
      GitHub webhook payload를 구조화된 데이터로 변환

      Args:
          webhook_payload: GitHub webhook raw data

      Returns:
          ParsedWebhookData: 구조화된 커밋 및 저장소 정보
      """
      repository_info = extract_repository_info(webhook_payload)
      commits_data = []

      for commit in webhook_payload.get('commits', []):
          commit_data = CommitData(
              sha=commit['id'],
              message=commit['message'],
              author=Author(
                  name=commit['author']['name'],
                  email=commit['author']['email'],
                  username=commit['author'].get('username')
              ),
              timestamp=parse_iso_datetime(commit['timestamp']),
              added_files=commit.get('added', []),
              modified_files=commit.get('modified', []),
              removed_files=commit.get('removed', [])
          )
          commits_data.append(commit_data)

      return ParsedWebhookData(
          repository=repository_info,
          commits=commits_data,
          event_type=webhook_payload.get('event_type', 'push')
      )

  ```
  ### 알고리즘 2: Diff Processing
  ```python
  def process_diff_data(commit_sha: str, diff_text: str) -> List[FileDiff]:
      """
      Git diff 텍스트를 파싱하여 파일별 변경사항 분석

      Args:
          commit_sha: 커밋 해시
          diff_text: Git diff 원시 텍스트

      Returns:
          List[FileDiff]: 파일별 변경사항 목록
      """
      diff_parser = DiffParser()
      parsed_diff = diff_parser.parse(diff_text)

      file_diffs = []
      for file_data in parsed_diff.files:
          language = detect_language(file_data.path)

          file_diff = FileDiff(
              file_path=file_data.path,
              additions=file_data.additions,
              deletions=file_data.deletions,
              is_binary=file_data.is_binary,
              language=language,
              hunks=process_hunks(file_data.hunks, language)
          )
          file_diffs.append(file_diff)

      return file_diffs

  ```
  ## 성능 고려사항
  - **병목 지점**: 대용량 diff 파싱, tree-sitter 구문 분석
  - **최적화 방안**:
  - **확장성**:
  ---
  # 🧪 테스트 계획
  ## 단위 테스트
  GitDataParser.parse_webhook_data() 테스트
  DiffProcessor.process_diff_data() 테스트
  언어별 코드 분석 테스트
  에러 케이스 처리 테스트
  ## 통합 테스트
  전체 파싱 플로우 테스트
  다양한 GitHub 이벤트 타입 테스트
  대용량 diff 처리 성능 테스트
  ## 테스트 데이터
  | 테스트 케이스 | 입력 데이터 | 예상 결과 | 우선순위 | 상태 |
  | --- | --- | --- | --- | --- |
  | 기본 push 이벤트 | sample_push_webhook.json | 정상 파싱 | High | Pending |
  | 대용량 diff | large_diff_webhook.json | 성능 최적화 | Medium | Pending |
  | 바이너리 파일 변경 | binary_change_webhook.json | 바이너리 감지 | Medium | Pending |
  | 다국어 코드 | multilang_webhook.json | 언어별 분석 | Low | Pending |
  ---
  # 📝 구현 체크리스트
  ## Phase 1: 기본 구조
  프로젝트 셋업 및 의존성 설치
  Pydantic 모델 정의 (CommitData, FileDiff, etc.)
  GitDataParser 클래스 기본 틀 구현
  ## Phase 2: 핵심 로직
  Webhook payload 파싱 로직 구현
  Git diff 추출 및 기본 분석
  DiffProcessor 클래스 구현
  언어별 코드 분석 로직 추가
  ## Phase 3: 최적화 & 테스트
  성능 최적화 (비동기 처리, 캐싱)
  단위 테스트 및 통합 테스트 작성
  에러 처리 및 로깅 강화
  API 문서화 완료
  ---
  # 🔗 관련 리소스
  ## 참조 문서
  - [CodePing.AI 프로젝트 기획서 - 보완된 버전](https://www.notion.so/CodePing-AI-21c18a4c52a1804ba78ddbcc2ba649d4)
  - [VIVECODING 모듈 상세설계서 작성 프롬프트](https://www.notion.so/VIVECODING-21c18a4c52a180878e1feef42a4a52e2)
  - [GitHub Webhooks API Documentation](https://docs.github.com/en/developers/webhooks-and-events/webhooks)
  ## 코드 저장소
  - **Repository**: CodePing.AI (예정)
  - **Branch**: feature/git-data-parser
  - **Main Files**:
  ## 협업 도구
  - **Issue Tracker**: GitHub Issues
  - **Communication**: Slack (#codeping-dev)
  - **Documentation**: Notion Wiki
  ---
  # 📊 진행 상황
  ## 현재 진행률
  - **전체 진행률**: 0% (설계 단계)
  - **설계 완료**: 100%
  - **구현 완료**: 0%
  - **테스트 완료**: 0%
  ## 다음 단계
  1. Python 개발 환경 설정 및 의존성 설치
  1. Pydantic 데이터 모델 정의
  1. GitDataParser 클래스 기본 구현
  ## 블로커 및 이슈
  - 대용량 diff 처리 시 메모리 사용량 최적화 필요
  - Tree-sitter 파서 설정의 복잡도
  - GitHub API 속도 제한 고려 필요
  ---
  26일 mvp 버전으로 구현 확정
  ## **🎯 GitDataParser MVP 설계안**
  **📋 MVP 범위 정의**
  **✅ MVP에 포함할 핵심 기능**
  1. **Webhook 데이터 파싱**: GitHub push 이벤트 기본 정보 추출
  1. **커밋 정보 구조화**: SHA, 메시지, 작성자, 타임스탬프
  1. **기본 Diff 처리**: 파일 변경 목록, 라인 수 통계
  1. **파일 변경사항 분석**: 추가/수정/삭제된 파일 목록
  1. **표준화된 데이터 모델**: 다운스트림 모듈과의 인터페이스
  1. 1. **Webhook 데이터 파싱**: GitHub push 이벤트 기본 정보 추출
  1. 1. **커밋 정보 구조화**: SHA, 메시지, 작성자, 타임스탬프
  1. 1. **기본 Diff 처리**: 파일 변경 목록, 라인 수 통계
  1. 1. **파일 변경사항 분석**: 추가/수정/삭제된 파일 목록
  1. 1. **표준화된 데이터 모델**: 다운스트림 모듈과의 인터페이스
  ### **🚫 MVP에서 제외할 고급 기능**
  - 코드 복잡도 계산 (tree-sitter, radon)
  - 언어별 구문 분석
  - 의미론적 코드 분석
  - 코드 패턴 감지
  - 성능 메트릭 수집
  ---
  *마지막 업데이트: 2025-06-26*

### 5. WebhookReceiver 모듈 상세설계서 - CodePing.AI (diff 분석 중심)
- **모듈 마스터**: 1개 연결
- **진행률**: 0.8
- **상태**: MVP 완료
- **예상작업시간**: 120
- **개발LLM**: o3
- **생성일**: 2025-06-24
- **개발자**: YunJeong Kim(관리자)
- **전체 프로젝트 마스터 DB**: 1개 연결
- **프롬프트**: 1개 연결
- **완료예정일**: 2025-06-25
- **우선순위**: 높음
- **이름**: WebhookReceiver 모듈 상세설계서 - CodePing.AI (diff 분석 중심)
- **링크**: [WebhookReceiver 모듈 상세설계서 - CodePing.AI (diff 분석 중심)](https://www.notion.so/WebhookReceiver-CodePing-AI-diff-21c18a4c52a181809189f5cdd6555379)

**📄 페이지 내용:**
  ### 전체 구조도 (하이브리드 방식 + diff 처리)
  ```plain text


  [GitHub Webhook with diff] → [PlatformRouter] → [Built-in Processors | Plugin Manager]
                                          ↓                    ↓                ↓
                                [Platform Detection]    [GitHub/GitLab]    [Extensions]
                                          ↓                    ↓                ↓
                                  [Core vs Extension]    [Fast Processing]  [Dynamic Loading]
                                          ↓                    ↓                ↓
                                  [DiffAnalyzer 연계]    [Complete Payload]  [Diff Extraction]

  ```
  ### 하이브리드 아키텍처 설계 (diff 정보 중심)
  ```plain text


  Core Platforms (Built-in)           Extension Platforms (Plugin)
  ┌─────────────────────────────┐       ┌─────────────────────────────┐
  │ • GitHub (diff 전체보존)    │       │ • Bitbucket                 │
  │ • GitLab (diff 전체보존)    │  VS   │ • Azure DevOps              │
  │ • (향후 1-2개 추가)         │       │ • Custom SCM                │
  │                             │       │ • Third-party               │
  │ 조건부 분기 (성능)          │       │ 플러그인 (확장성)           │
  │ + diff 정보 완전 보존       │       │ + diff 정보 표준화          │
  └─────────────────────────────┘       └─────────────────────────────┘

  ```
  ### 기술 스택 (diff 처리 추가)
  - **프론트엔드**: 해당없음 (백엔드 전용 모듈)
  - **백엔드**: Python 3.11+, FastAPI, Pydantic
  - **데이터베이스**: PostgreSQL 16, SQLAlchemy ORM
  - **핵심 SCM**: GitHub, GitLab (빌트인) + **diff 정보 완전 보존**
  - **확장 SCM**: Plugin 시스템 (동적 로딩) + **diff 정보 표준화**
  - **diff 처리**: GitPython, python-diff-parser, tree-sitter
  - **기타**: uvicorn, pytest, importlib
  ---
  ## 📦 클래스 설계
  ### 클래스 1: PlatformRouter (하이브리드 라우터 + diff 인식)
  | 항목 | 내용 | 비고 |
  | --- | --- | --- |
  | 클래스명 | PlatformRouter | 하이브리드 방식 라우터 |
  | 상속/구현 | BaseRouter | 기본 라우터 인터페이스 |
  | 주요역할 | 플랫폼 감지 및 라우팅 | diff 정보 포함 |
  | 성능목표 | <50ms 응답시간 | 핵심 플랫폼 기준 |
  | 확장성 | Plugin 지원 | 동적 로딩 |
  ### 역할과 책임 (diff 정보 처리 추가)
  - HTTP 요청에서 SCM 플랫폼 자동 감지
  - **diff 정보 포함 여부 확인 및 보존**
  - 핵심 플랫폼 vs 확장 플랫폼 분류
  - 적절한 처리기로 라우팅 (빌트인 vs 플러그인)
  - **전체 webhook payload 보존 (diff 정보 포함)**
  - 통합된 응답 인터페이스 제공
  ### Input 데이터 (diff 정보 추가)
  | 필드명 | 타입 | 설명 | 필수여부 |
  | --- | --- | --- | --- |
  | request | HTTPRequest | 전체 webhook 요청 | 필수 |
  | headers | Dict[str, str] | HTTP 헤더 정보 | 필수 |
  | payload | Dict[str, Any] | 전체 payload (diff 포함) | 필수 |
  | platform_hint | Optional[str] | 플랫폼 힌트 | 선택 |
  ### Output 데이터 (diff 정보 포함)
  | 필드명 | 타입 | 설명 | 포함내용 |
  | --- | --- | --- | --- |
  | routing_info | RoutingInfo | 라우팅 결과 | 플랫폼, 처리기 |
  | complete_payload | Dict[str, Any] | 전체 payload 보존 | diff 정보 포함 |
  | platform | str | 감지된 플랫폼 | github, gitlab 등 |
  | processor_type | str | 처리기 유형 | core, extension |
  ### 처리 플로우 (diff 정보 중심)
  ## 초기화
  1. 핵심 플랫폼 목록 설정 ('github', 'gitlab')
  1. 플러그인 매니저 초기화
  1. 플랫폼 감지 규칙 로드
  1. **diff 정보 보존 설정 확인**
  ## 데이터 처리
  1. HTTP 헤더 기반 플랫폼 감지
  1. **diff 정보 포함 여부 확인**
  1. 핵심 플랫폼 여부 판단
  1. 적절한 처리기로 라우팅
  1. **전체 payload 보존 (diff 정보 포함)**
  1. 성능 메트릭 수집
  ## 결과 반환
  1. 라우팅 정보 반환
  1. **완전한 payload 데이터 전달**
  1. 처리 결과 통합 포맷 적용
  ### 핵심 메서드 (diff 처리 추가)
  - `detect_platform()`: 플랫폼 자동 감지
  - `is_core_platform()`: 핵심 플랫폼 여부 판단
  - `route_to_processor()`: 적절한 처리기로 라우팅
  - `unify_response()`: 응답 형식 통합
  - `**preserve_diff_data()**`**: diff 정보 보존**
  - `**validate_diff_completeness()**`**: diff 완전성 검증**
  ---
  ### 클래스 2: CorePlatformProcessor (빌트인 처리기 + diff 분석)
  ### 역할과 책임 (diff 처리 중심)
  - GitHub, GitLab 등 핵심 플랫폼 전용 고성능 처리
  - **코드 변경사항(diff) 상세 분석 및 보존**
  - 조건부 분기를 통한 최적화된 처리 로직
  - 플랫폼별 특화 기능 구현
  - **diff 정보를 포함한 완전한 데이터 추출**
  - 빠른 응답 시간 보장
  ### Input 데이터 (diff 정보 중심)
  | 필드명 | 타입 | 설명 | diff 관련 |
  | --- | --- | --- | --- |
  | platform | str | 플랫폼 식별자 | github, gitlab |
  | raw_payload | Dict[str, Any] | 원본 webhook payload | diff 정보 포함 |
  | headers | Dict[str, str] | HTTP 헤더 | 서명 검증용 |
  | config | PlatformConfig | 플랫폼 설정 | diff 처리 옵션 |
  | diff_options | DiffOptions | diff 처리 옵션 | 새로 추가 |
  ### Output 데이터 (diff 포함)
  | 필드명 | 타입 | 설명 | diff 관련 |
  | --- | --- | --- | --- |
  | processed_data | ProcessedData | 처리된 데이터 | diff 정보 포함 |
  | validation_result | bool | 검증 결과 | 서명 등 |
  | performance_metrics | Dict | 성능 지표 | 처리 시간 등 |
  | diff_summary | DiffSummary | diff 요약 정보 | 새로 추가 |
  ### 처리 플로우 (diff 중심)
  ## 초기화
  1. GitHub/GitLab 전용 검증기 초기화
  1. 플랫폼별 최적화된 파서 로드
  1. **diff 분석기 초기화**
  1. 성능 모니터링 설정
  ## 데이터 처리
  1. 플랫폼별 조건부 분기 실행
  1. 최적화된 서명 검증
  1. **diff 정보 상세 분석 및 추출**
  1. 고성능 데이터 파싱
  1. 플랫폼 특화 기능 처리
  1. **코드 변경사항 의미론적 분석**
  ## 결과 반환
  1. 표준 형식으로 데이터 반환
  1. **diff 분석 결과 포함**
  1. 성능 메트릭 포함
  ### 핵심 메서드 (diff 처리 추가)
  - `process_github()`: GitHub 전용 최적화 처리
  - `process_gitlab()`: GitLab 전용 최적화 처리
  - `verify_core_signature()`: 핵심 플랫폼 서명 검증
  - `parse_core_payload()`: 고성능 페이로드 파싱
  - `**extract_diff_data()**`**: diff 정보 추출**
  - `**analyze_code_changes()**`**: 코드 변경사항 분석**
  ---
  ### 클래스 3: ExtensionPluginManager (플러그인 관리자 + diff 표준화)
  ### 역할과 책임 (diff 표준화 추가)
  - 확장 플랫폼용 플러그인 동적 로딩
  - **다양한 플랫폼의 diff 정보 표준화**
  - 플러그인 라이프사이클 관리
  - 플러그인 간 표준 인터페이스 강제
  - **diff 정보 통합 형식 제공**
  - 플러그인 설정 및 보안 관리
  ### Input 데이터 (diff 표준화)
  | 필드명 | 타입 | 설명 | diff 관련 |
  | --- | --- | --- | --- |
  | plugin_name | str | 플러그인 식별자 | bitbucket, azure 등 |
  | raw_data | Dict[str, Any] | 원본 데이터 | 플랫폼별 diff 형식 |
  | plugin_config | Dict | 플러그인 설정 | diff 처리 옵션 |
  | security_context | SecurityContext | 보안 컨텍스트 | 플러그인 권한 |
  | diff_schema | DiffSchema | diff 스키마 | 표준화 규칙 |
  ### Output 데이터 (표준화된 diff)
  | 필드명 | 타입 | 설명 | diff 관련 |
  | --- | --- | --- | --- |
  | standardized_data | StandardData | 표준화된 데이터 | 통합 diff 형식 |
  | plugin_status | PluginStatus | 플러그인 상태 | 성공/실패 |
  | processing_log | List[str] | 처리 로그 | 디버깅용 |
  | normalized_diff | NormalizedDiff | 정규화된 diff | 표준 형식 |
  ### 처리 플로우 (diff 표준화 중심)
  ## 초기화
  1. 플러그인 디렉토리 스캔
  1. IExtensionPlugin 인터페이스 확인
  1. **diff 표준화 스키마 로드**
  1. 플러그인 메타데이터 로드
  1. 보안 검증 및 등록
  ## 데이터 처리
  1. 요청된 플러그인 검색
  1. **플랫폼별 diff 형식 인식**
  1. 플러그인 동적 로딩
  1. 플러그인 메서드 호출
  1. **diff 정보 표준화 변환**
  1. 에러 처리 및 폴백
  ## 결과 반환
  1. 플러그인 처리 결과 반환
  1. **표준화된 diff 정보 제공**
  1. 실패시 기본 처리기 사용
  ### 핵심 메서드 (diff 표준화 추가)
  - `load_extension_plugin()`: 확장 플러그인 동적 로딩
  - `validate_plugin_interface()`: 플러그인 인터페이스 검증
  - `**standardize_diff_format()**`**: diff 형식 표준화**
  - `**normalize_code_changes()**`**: 코드 변경사항 정규화**
  - `manage_plugin_lifecycle()`: 플러그인 생명주기 관리
  - `enforce_security_policy()`: 보안 정책 강제
  ---
  ## 🔄 수정된 데이터 흐름 (diff 중심)
  ```plain text


  GitHub Webhook (with complete diff)
      ↓
  [WebhookReceiver] - 전체 payload 보존 (diff 정보 포함)
      ↓
  [PlatformRouter] - 플랫폼 감지 + diff 정보 보존
      ↓
  [CorePlatformProcessor] - diff 상세 분석 및 추출
      ↓
  [DiffAnalyzer 연계] - 코드 변경사항 의미론적 분석
      ↓
  [GitDataParser 전달] - 구조화된 diff 데이터 생성

  ```
  ---
  ## 🎯 diff 처리 핵심 요구사항
  ### WebhookReceiver의 diff 처리 책임
  1. **완전한 payload 보존**: GitHub webhook의 모든 diff 정보 손실 없이 보존
  1. **diff 정보 감지**: webhook에 diff 정보 포함 여부 자동 감지
  1. **플랫폼별 차별화**: GitHub과 GitLab의 diff 형식 차이 인식
  1. **표준화 준비**: 다음 모듈(GitDataParser)에서 처리할 수 있는 형태로 전달
  1. **성능 최적화**: 대용량 diff 정보 처리 시 메모리 효율성 보장
  ### 추가된 핵심 기능
  - **DiffPreservationManager**: diff 정보 완전 보존 관리
  - **PayloadValidator**: diff 포함 payload 검증
  - **PlatformDiffDetector**: 플랫폼별 diff 형식 감지
  - **MemoryOptimizer**: 대용량 diff 처리 최적화
  이 설계서는 **최신 기획서의 diff 분석 중심 요구사항**을 완전히 반영하여, WebhookReceiver 모듈이 코드 변경사항을 완전히 보존하고 다음 단계 분석을 위한 기반을 제공할 수 있도록 설계되었습니다.
